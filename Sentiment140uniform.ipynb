{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d30495-da30-4f10-92a9-6d79d321192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Didn`t work for me  Except when I used the wo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in santa clara a long way from hoe, well not t...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mine pools... just in someone elses pocket</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMGoodness back to school soon FUN!</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence     Label\n",
       "0   Didn`t work for me  Except when I used the wo...  Negative\n",
       "1  in santa clara a long way from hoe, well not t...   Neutral\n",
       "2                                          THANK YOU  Positive\n",
       "3         mine pools... just in someone elses pocket   Neutral\n",
       "4                OMGoodness back to school soon FUN!  Positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with ISO-8859-1 encoding\n",
    "df = pd.read_csv('balanced_train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d49ce7d-150d-42ef-8e8d-a2dcc3e4d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts of each Label value:\n",
      "Label\n",
      "Positive    8582\n",
      "Negative    7781\n",
      "Neutral     7781\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show counts of each unique value in the 'Label' column\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(\"\\nCounts of each Label value:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4360a356-7ba2-4838-a66d-4d772d041793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated counts of each Label value:\n",
      "Label\n",
      "Negative    9754\n",
      "Neutral     9426\n",
      "Positive    8582\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the balanced_train CSV file\n",
    "df_balanced = pd.read_csv('balanced_train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Load the combined_sentences_labeled CSV file\n",
    "df_combined = pd.read_csv('combined_sentences_labeled.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Filter out Neutral and Negative sentences from combined_sentences_labeled\n",
    "df_combined_filtered = df_combined[df_combined['Label'].isin(['Neutral', 'Negative'])]\n",
    "\n",
    "# Append the filtered Neutral and Negative sentences to the balanced_train dataset\n",
    "df_updated = pd.concat([df_balanced, df_combined_filtered[['Sentence', 'Label']]], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_updated.to_csv('balanced_train_updated.csv', index=False, encoding='ISO-8859-1')\n",
    "\n",
    "# Display the counts of each Label value to verify\n",
    "label_counts_updated = df_updated['Label'].value_counts()\n",
    "print(\"\\nUpdated counts of each Label value:\")\n",
    "print(label_counts_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc878da-b38b-4e93-a506-ca4e01f5d1c9",
   "metadata": {},
   "source": [
    "# LETS START!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4990fa5c-3a43-4647-a493-b6f22d99f74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Didn`t work for me  Except when I used the wo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in santa clara a long way from hoe, well not t...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mine pools... just in someone elses pocket</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMGoodness back to school soon FUN!</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence     Label\n",
       "0   Didn`t work for me  Except when I used the wo...  Negative\n",
       "1  in santa clara a long way from hoe, well not t...   Neutral\n",
       "2                                          THANK YOU  Positive\n",
       "3         mine pools... just in someone elses pocket   Neutral\n",
       "4                OMGoodness back to school soon FUN!  Positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with ISO-8859-1 encoding\n",
    "df = pd.read_csv('balanced_train_updated.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7edd85c6-5470-4e57-ae95-cb4eafcd48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated counts of each Label value:\n",
      "Label\n",
      "Negative    9754\n",
      "Neutral     9426\n",
      "Positive    8582\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the counts of each Label value to verify\n",
    "label_counts_updated = df_updated['Label'].value_counts()\n",
    "print(\"\\nUpdated counts of each Label value:\")\n",
    "print(label_counts_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d189f3-a7a1-48b6-bfb7-3540e6751342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in each column:\n",
      " Sentence    1\n",
      "Label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the DataFrame\n",
    "nan_summary = df.isna().sum()\n",
    "\n",
    "# Display the count of NaN values for each column\n",
    "print(\"NaN values in each column:\\n\", nan_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7ea0da-3323-4419-8eea-ea362243ac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN values in each column after cleaning:\n",
      " Sentence    0\n",
      "Label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NaN values in the 'text' or 'sentiment' columns\n",
    "df_subset_cleaned = df.dropna(subset=['Sentence', 'Label',])\n",
    "\n",
    "# Verify that NaN values have been removed\n",
    "nan_summary_after = df_subset_cleaned.isna().sum()\n",
    "print(\"\\nNaN values in each column after cleaning:\\n\", nan_summary_after)\n",
    "\n",
    "df=df_subset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b6d2f4-b1ae-4466-b862-45a5cc81d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts after removing NaN values:\n",
      " Label\n",
      "Negative    9754\n",
      "Neutral     9425\n",
      "Positive    8582\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values and their counts in the 'sentiment' column after cleaning\n",
    "sentiment_counts_cleaned = df_subset_cleaned['Label'].value_counts()\n",
    "\n",
    "\n",
    "# Display the counts of each sentiment\n",
    "print(\"Sentiment counts after removing NaN values:\\n\", sentiment_counts_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b83356-4043-43c2-b204-9b1b81278ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>May the 4th be with you  #starwarsday (via )</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24614</th>\n",
       "      <td>She finished her report.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>ahaha i know. but now i can`t do anything ove...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17838</th>\n",
       "      <td>Gonna nap n chill then probably go to the movi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>http://twitpic.com/4i2zu - wow, thats so cool!</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25333</th>\n",
       "      <td>She analyzed the proposal, recognizing both st...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25748</th>\n",
       "      <td>You explored the system, finding both function...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>I wanna go to the extra show really bad</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>Just rang the irish one. Drunk. Must confiscat...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>The party has to be moved to next weekend  .....</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence     Label\n",
       "2384        May the 4th be with you  #starwarsday (via )   Neutral\n",
       "24614                           She finished her report.   Neutral\n",
       "3818    ahaha i know. but now i can`t do anything ove...  Negative\n",
       "17838  Gonna nap n chill then probably go to the movi...  Negative\n",
       "1329      http://twitpic.com/4i2zu - wow, thats so cool!  Positive\n",
       "25333  She analyzed the proposal, recognizing both st...   Neutral\n",
       "25748  You explored the system, finding both function...   Neutral\n",
       "1086             I wanna go to the extra show really bad  Negative\n",
       "11894  Just rang the irish one. Drunk. Must confiscat...  Negative\n",
       "1189    The party has to be moved to next weekend  .....  Negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c1c7e-cd8b-4125-94d9-cbbb666d0c08",
   "metadata": {},
   "source": [
    "# TRANSFORMATIONS APPLIED TO TEXT/SENTENCE:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982ab93f-93e8-4037-a686-af28c4ce5242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a set of stopwords\n",
    "stopwords = set([\"a\", \"an\", \"the\", \"is\", \"in\", \"of\", \"to\", \"and\", \"for\", \"with\", \"on\", \"at\", \"by\", \"it\", \"this\", \"that\", \"which\", \"who\", \"whom\", \"has\", \"have\", \"had\", \"will\", \"would\", \"can\", \"could\", \"should\", \"may\", \"might\", \"there\", \"where\", \"how\"])\n",
    "\n",
    "# Function to get wordnet POS tags for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to handle elongated words (e.g., reallllllly -> really)\n",
    "def reduce_elongation(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)  # Keeps two of the repeated letters\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        # 1. Expand contractions\n",
    "        text = contractions.fix(text)\n",
    "        \n",
    "        # 2. Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 3. Replace elongated words (e.g., \"sooooo\" -> \"soo\")\n",
    "        text = ' '.join([reduce_elongation(word) for word in text.split()])\n",
    "        \n",
    "        # 4. Remove special characters (e.g., ãââ½)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        \n",
    "        # 5. Remove HTTP URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # 6. Remove special characters, punctuation, and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        \n",
    "        # 7. Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # 8. Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 9. Remove short words (length < 2)\n",
    "        words = [word for word in words if len(word) >= 2]\n",
    "        \n",
    "        # 10. Remove stopwords\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        \n",
    "        # 11. Lemmatize the words based on POS tagging\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "        \n",
    "        # Join the words back into a single string\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    return text  # If not a string, return the original value\n",
    "\n",
    "# Load your DataFrame (assuming df1 is already loaded)\n",
    "# Apply the cleaning function to the 'text' column and create 'cleaned_text'\n",
    "df['cleaned_text'] = df['Sentence'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6172ca49-8721-4f0c-8e72-17046fb5d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                          Sentence  \\\n",
      "23060                                                                                         Totally just skid all over the road.   \n",
      "8303                                                                         transcribing my tenth (and last) report for the night   \n",
      "2828                                                                      monday at the school  gosshhh what I`ve been waiting for   \n",
      "11831   I had fun tonight! I`ll leave you with this...Brandi Carlile will be in Dallas today.  We should just sneak into the show!   \n",
      "13406        Happy Mother`s Day to all the amazing women who put up with us crazy, demanding children. Thank you.  Very very much.   \n",
      "19983                                                                                   awwww bless her  she needs another chance.   \n",
      "17838                   Gonna nap n chill then probably go to the movie later. Ugh i have a headache this sux ****. Cloudy day too   \n",
      "25467                                       They evaluated the project, finding both success and areas needing further refinement.   \n",
      "17530                                                          Trying to find a foreign place in a foreign town  i have being lost   \n",
      "21716                                                                  cool thanks for the response.  I`ll check out that Firefox!   \n",
      "20970                                                                                 snotty nose  poorly chest! this is not good!   \n",
      "2126                    I think I would be a good radio dj...I like awesome music and I have a great personality!!!!  ;) !!!   !!!   \n",
      "16778    I kind of figured. He`d probably be unable to reason out why a lot of the things in manga happen, or the way they happen.   \n",
      "20661             have to avoid the burning desire to say I already have one  no, no room for piglets, just enough room for chucks   \n",
      "12918                  The last day of the last time I visit my parents as an unmarried woman  (85/21589) ( http://bit.ly/Mo0QN  )   \n",
      "6301                                                   Have a fantastic opening flower drummers!! From the ALL Asian cast and crew   \n",
      "18481                                                                                             Is watching the final Underbelly   \n",
      "22487                                                                                                              Work is a no go   \n",
      "5845      Shiraz event was in an `anti-bahai` center.not in a mosque or in election campaign.Sadly children wr involved in zahedan   \n",
      "17765                             I miss you too, Mojokins! I go there, but not long enough to comment  Will do more this weekend!   \n",
      "\n",
      "                                                                                        cleaned_text  \\\n",
      "23060                                                                totally just skid all over road   \n",
      "8303                                                           transcribe my tenth last report night   \n",
      "2828                                                           monday school gosshh what ive be wait   \n",
      "11831           fun tonight ill leave you thisbrandi carlile be dallas today we just sneak into show   \n",
      "13406          happy mother day all amaze woman put up u crazy demand child thank you very very much   \n",
      "19983                                                          aww bless her she need another chance   \n",
      "17838                     go nap chill then probably go movie later ugh headache suck cloudy day too   \n",
      "25467                               they evaluate project find both success area need far refinement   \n",
      "17530                                                    try find foreign place foreign town be lose   \n",
      "21716                                                     cool thanks response ill check out firefox   \n",
      "20970                                                              snotty nose poorly chest not good   \n",
      "2126                                    think be good radio dji like awesome music great personality   \n",
      "16778    kind figure hed probably be unable reason out why lot thing manga happen or way they happen   \n",
      "20661                  avoid burning desire say already one no no room piglet just enough room chuck   \n",
      "12918                                           last day last time visit my parent a unmarried woman   \n",
      "6301                                       fantastic opening flower drummer from all asian cast crew   \n",
      "18481                                                                         watch final underbelly   \n",
      "22487                                                                                     work no go   \n",
      "5845   shiraz event be antibahai centernot mosque or election campaignsadly child wr involve zahedan   \n",
      "17765                           miss you too mojokins go but not long enough comment do more weekend   \n",
      "\n",
      "          Label  \n",
      "23060   Neutral  \n",
      "8303    Neutral  \n",
      "2828   Positive  \n",
      "11831  Positive  \n",
      "13406  Positive  \n",
      "19983  Positive  \n",
      "17838  Negative  \n",
      "25467   Neutral  \n",
      "17530  Negative  \n",
      "21716  Positive  \n",
      "20970  Negative  \n",
      "2126   Positive  \n",
      "16778  Negative  \n",
      "20661   Neutral  \n",
      "12918   Neutral  \n",
      "6301   Positive  \n",
      "18481   Neutral  \n",
      "22487   Neutral  \n",
      "5845   Negative  \n",
      "17765  Negative  \n"
     ]
    }
   ],
   "source": [
    "# Set pandas to display the full content of the columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df[['Sentence', 'cleaned_text', 'Label']].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa3e899-2d3f-4c47-980e-9652c662016f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Didn`t work for me  Except when I used the word autofollow and got followed by an bot selling an autofollow program.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>didnt work me except when use word autofollow get follow bot sell autofollow program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in santa clara a long way from hoe, well not that far. it sure seems like it.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>santa clara long way from hoe well not far sure seem like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THANK YOU</td>\n",
       "      <td>Positive</td>\n",
       "      <td>thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mine pools... just in someone elses pocket</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>mine pool just someone elses pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMGoodness back to school soon FUN!</td>\n",
       "      <td>Positive</td>\n",
       "      <td>omgoodness back school soon fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27757</th>\n",
       "      <td>Itâs wonderful how often you share, but the posts are lacking in variety.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it wonderful often you share but post be lack variety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27758</th>\n",
       "      <td>The visuals are striking, but the actual post is quite flat.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>visuals be strike but actual post quite flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27759</th>\n",
       "      <td>Itâs great how polished your posts look, but theyâre not always captivating.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it great polished your post look but theyre not always captivate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27760</th>\n",
       "      <td>The design is sleek, but the content is somewhat boring.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>design sleek but content somewhat boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27761</th>\n",
       "      <td>Itâs amazing how much thought goes into each post, but the content often falls short.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it amaze much thought go into each post but content often fall short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27761 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    Sentence  \\\n",
       "0       Didn`t work for me  Except when I used the word autofollow and got followed by an bot selling an autofollow program.   \n",
       "1                                              in santa clara a long way from hoe, well not that far. it sure seems like it.   \n",
       "2                                                                                                                  THANK YOU   \n",
       "3                                                                                 mine pools... just in someone elses pocket   \n",
       "4                                                                                        OMGoodness back to school soon FUN!   \n",
       "...                                                                                                                      ...   \n",
       "27757                                            Itâs wonderful how often you share, but the posts are lacking in variety.   \n",
       "27758                                                           The visuals are striking, but the actual post is quite flat.   \n",
       "27759                                       Itâs great how polished your posts look, but theyâre not always captivating.   \n",
       "27760                                                               The design is sleek, but the content is somewhat boring.   \n",
       "27761                                Itâs amazing how much thought goes into each post, but the content often falls short.   \n",
       "\n",
       "          Label  \\\n",
       "0      Negative   \n",
       "1       Neutral   \n",
       "2      Positive   \n",
       "3       Neutral   \n",
       "4      Positive   \n",
       "...         ...   \n",
       "27757  Negative   \n",
       "27758  Negative   \n",
       "27759  Negative   \n",
       "27760  Negative   \n",
       "27761  Negative   \n",
       "\n",
       "                                                                               cleaned_text  \n",
       "0      didnt work me except when use word autofollow get follow bot sell autofollow program  \n",
       "1                                 santa clara long way from hoe well not far sure seem like  \n",
       "2                                                                                 thank you  \n",
       "3                                                       mine pool just someone elses pocket  \n",
       "4                                                           omgoodness back school soon fun  \n",
       "...                                                                                     ...  \n",
       "27757                                 it wonderful often you share but post be lack variety  \n",
       "27758                                          visuals be strike but actual post quite flat  \n",
       "27759                      it great polished your post look but theyre not always captivate  \n",
       "27760                                              design sleek but content somewhat boring  \n",
       "27761                  it amaze much thought go into each post but content often fall short  \n",
       "\n",
       "[27761 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed507f15-0151-42e7-9a69-669552a73f09",
   "metadata": {},
   "source": [
    "# Example of How Glove Embedding works to convert text into mulidimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fd193ac-f21d-420c-a117-1c1e79c32ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Path to GloVe embeddings file (e.g., 'glove.6B.100d.txt')\n",
    "glove_file_path = 'glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed42ad6-b285-46ef-b960-d43051b23d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Modify the sentence_to_embedding function to handle NaN or non-string values\n",
    "def sentence_to_embedding(sentence, embeddings):\n",
    "    # Ensure the input is a valid string; if not, convert it to an empty string\n",
    "    if not isinstance(sentence, str):\n",
    "        sentence = \"\"\n",
    "        \n",
    "    words = word_tokenize(sentence)\n",
    "    valid_words = [embeddings[word] for word in words if word in embeddings]\n",
    "    \n",
    "    if valid_words:\n",
    "        return np.mean(valid_words, axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)  # Assuming 100-dimensional GloVe embeddings\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "embedding_matrix = np.array([sentence_to_embedding(text, glove_embeddings) for text in df['cleaned_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3279ce0e-7e3b-45a8-ac4a-ce94c9a434d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followfriday', 'thank', 'you', 'much', 'behind', 'still', 'about', 'half', 'what']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"followfriday thank you much behind still about half what\")\n",
    "# Print the tokens\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af375d21-cf1d-4783-95e5-286764455025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words present in GloVe embeddings: ['thank', 'you', 'much', 'behind', 'still', 'about', 'half', 'what']\n",
      "Words absent from GloVe embeddings: ['followfriday']\n",
      "Embeddings for present words:\n",
      "Word: thank, Embedding: [-0.056244  0.55972   0.4774   -0.22186   0.020482]...\n",
      "Word: you, Embedding: [-0.49886  0.76602  0.89751 -0.78547 -0.6855 ]...\n",
      "Word: much, Embedding: [-0.3384   0.6032   0.61412 -0.05686 -0.37309]...\n",
      "Word: behind, Embedding: [-0.17607  0.32129  0.59174 -0.48619  0.34921]...\n",
      "Word: still, Embedding: [-0.04248  0.80249  0.51451 -0.55427 -0.13799]...\n",
      "Word: about, Embedding: [ 0.66039  0.63888  0.86264  0.27455 -0.89222]...\n",
      "Word: half, Embedding: [ 0.11202  0.60857  0.35559 -0.47541 -0.1657 ]...\n",
      "Word: what, Embedding: [-0.1518   0.38409  0.8934  -0.42421 -0.92161]...\n"
     ]
    }
   ],
   "source": [
    "# Create lists to hold present and absent words\n",
    "present_words = []\n",
    "absent_words = []\n",
    "\n",
    "# Check for each word if it is in glove_embeddings\n",
    "for word in words:\n",
    "    if word in glove_embeddings:\n",
    "        present_words.append(word)\n",
    "    else:\n",
    "        absent_words.append(word)\n",
    "\n",
    "# Extract embeddings for present words\n",
    "valid_words = [glove_embeddings[word] for word in present_words]\n",
    "\n",
    "# Print results\n",
    "print(\"Words present in GloVe embeddings:\", present_words)\n",
    "print(\"Words absent from GloVe embeddings:\", absent_words)\n",
    "print(\"Embeddings for present words:\")\n",
    "for word, embedding in zip(present_words, valid_words):\n",
    "    print(f\"Word: {word}, Embedding: {embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40a3175-eaeb-4659-9498-5bce7863fab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.14304952e-02  5.85532486e-01  6.50863767e-01 -3.41215014e-01\n",
      " -3.50802243e-01  3.79183263e-01 -1.88866466e-01  1.31394997e-01\n",
      " -3.80120017e-02 -2.49909148e-01  4.29248720e-01  2.14636013e-01\n",
      "  1.24009117e-01 -1.37242079e-01  1.65671244e-01 -4.16303992e-01\n",
      " -1.09373137e-01  1.26152009e-01 -3.27242523e-01  4.08639610e-01\n",
      "  3.11282247e-01  3.12231362e-01 -9.66031253e-02 -2.23210514e-01\n",
      "  2.42671266e-01  8.28860328e-04 -2.34162509e-01 -4.27338243e-01\n",
      "  1.14087753e-01 -4.32371974e-01 -4.54836190e-02  4.14996237e-01\n",
      "  1.38813615e-01  2.77333464e-02 -6.83845058e-02  2.39871264e-01\n",
      " -4.22142446e-02  3.49828988e-01  1.08415015e-01 -1.85637623e-01\n",
      " -3.38328242e-01 -2.53160000e-01  3.06167513e-01 -2.31215000e-01\n",
      " -3.47438484e-01 -5.73085025e-02  2.38317490e-01 -4.17411238e-01\n",
      " -8.39576274e-02 -9.71107483e-01  2.09547937e-01 -5.71529679e-02\n",
      "  4.11347561e-02  1.17401505e+00 -2.92947501e-01 -2.46105266e+00\n",
      " -3.95629965e-02 -1.78059638e-01  1.41188133e+00  4.13980007e-01\n",
      "  7.49856308e-02  1.03161883e+00 -2.68907130e-01 -2.87610032e-02\n",
      "  5.96035004e-01  1.35667861e-01  7.04069972e-01  4.67007518e-01\n",
      "  5.89193702e-02 -5.10607474e-02  1.58615589e-01 -7.44528770e-02\n",
      " -8.04249942e-02 -3.95611465e-01  1.76502362e-01  1.37421981e-01\n",
      "  3.33756208e-04 -1.56203866e-01 -6.70017004e-01  1.39031380e-01\n",
      "  5.30846238e-01  3.67826223e-02 -6.57795012e-01  6.93831667e-02\n",
      " -1.42533386e+00 -2.79917002e-01 -9.98816341e-02 -7.23436624e-02\n",
      " -3.55338752e-01 -3.62140268e-01  6.79409951e-02 -1.09228961e-01\n",
      " -7.32463747e-02 -1.39678106e-01 -6.73753738e-01 -2.62908995e-01\n",
      " -1.69734880e-01 -2.23170385e-01  3.78289998e-01  4.08087522e-01]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"followfriday thank you much behind still about half what\"\n",
    "\n",
    "# Convert sentence to embedding\n",
    "embedding = sentence_to_embedding(sentence, glove_embeddings)\n",
    "\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44bb6c9b-86a1-47a4-bf3a-223181fb3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'thank': ['grateful', 'congratulations', 'thanking', 'me', 'wish']\n",
      "Similar words to 'you': [\"'ll\", \"n't\", 'know', 'i', 'do']\n",
      "Similar words to 'much': ['even', 'more', 'less', 'so', 'too']\n",
      "Similar words to 'behind': ['away', 'while', 'back', 'out', 'ahead']\n",
      "Similar words to 'still': ['now', 'already', 'though', 'even', 'but']\n",
      "Similar words to 'about': ['some', 'than', 'much', 'just', 'there']\n",
      "Similar words to 'half': ['second', 'third', 'over', 'five', 'almost']\n",
      "Similar words to 'what': ['how', 'why', 'fact', 'know', 'that']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of similar words to retrieve\n",
    "N = 5\n",
    "\n",
    "# Create a dictionary to store the similar words\n",
    "similar_words = {}\n",
    "\n",
    "# Convert the dictionary values to a numpy array for efficient computation\n",
    "embeddings_array = np.array(list(glove_embeddings.values()))\n",
    "word_list = list(glove_embeddings.keys())\n",
    "\n",
    "# Iterate over the present words\n",
    "for word in present_words:\n",
    "    # Get the embedding of the current word\n",
    "    word_embedding = glove_embeddings[word]\n",
    "    \n",
    "    # Calculate the cosine similarity with all words in the GloVe embeddings\n",
    "    similarities = np.dot(embeddings_array, word_embedding) / (np.linalg.norm(embeddings_array, axis=1) * np.linalg.norm(word_embedding))\n",
    "    \n",
    "    # Get the index of the current word in the embeddings list\n",
    "    word_index = word_list.index(word)\n",
    "    \n",
    "    # Set similarities of the word itself to -1 to exclude it\n",
    "    similarities[word_index] = -1\n",
    "    \n",
    "    # Get the top N similar words\n",
    "    top_similarities = np.argsort(-similarities)[:N]\n",
    "    similar_words[word] = [word_list[i] for i in top_similarities]\n",
    "\n",
    "# Print the similar words\n",
    "for word, sims in similar_words.items():\n",
    "    print(f\"Similar words to '{word}': {sims}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da5e6b8-01da-4a3e-bf0b-5b9dd8a47b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'cleaned_text' is NaN or empty\n",
    "df = df.dropna(subset=['cleaned_text'])\n",
    "\n",
    "# Alternatively, you can remove rows where 'cleaned_text' is an empty string\n",
    "df = df[df['cleaned_text'].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e996d3-dfbb-4a02-8310-1cba756287d4",
   "metadata": {},
   "source": [
    "# COMBINING VADER(lexicon and rule-based sentiment analysis tool) And Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70eda684-5a0b-4b79-8ee7-c9ca771216bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get VADER sentiment scores\n",
    "def vader_sentiment_features(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return [scores['neg'], scores['neu'], scores['pos'], scores['compound']]\n",
    "\n",
    "# Apply to your dataset\n",
    "vader_features = np.array([vader_sentiment_features(text) for text in df['cleaned_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b373315-d0f2-4609-a06f-11ccf2dbe3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the feature matrix and target vector\n",
    "X = np.array([sentence_to_embedding(text, glove_embeddings) for text in df['cleaned_text']])\n",
    "y = df['Label']\n",
    "\n",
    "# Combine GloVe embeddings and VADER sentiment features\n",
    "X_combined = np.hstack([X, vader_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d5e0d5-5dd6-4a8e-9098-4b9eb0867695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Negative\n",
       "1         Neutral\n",
       "2        Positive\n",
       "3         Neutral\n",
       "4        Positive\n",
       "           ...   \n",
       "27757    Negative\n",
       "27758    Negative\n",
       "27759    Negative\n",
       "27760    Negative\n",
       "27761    Negative\n",
       "Name: Label, Length: 27756, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5040511a-4f4b-41ab-9ff7-97a6acfd54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Label'])  # e.g., 'negative' -> 0, 'neutral' -> 1, 'positive' -> 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3184b7c7-7767-4108-a351-f2b3ef8079f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f653d78-98c6-4ac7-84d5-837db9cbb607",
   "metadata": {},
   "source": [
    "**Using Different ML models to check which works best with our text(using GloVe embeddings and VADER sentiment features) to represent the input data in a form that is understandable by the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "640f8db1-99a1-4098-b664-3547cc01c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.74      0.73      1903\n",
      "     Neutral       0.67      0.63      0.65      1947\n",
      "    Positive       0.73      0.76      0.74      1703\n",
      "\n",
      "    accuracy                           0.71      5553\n",
      "   macro avg       0.71      0.71      0.71      5553\n",
      "weighted avg       0.71      0.71      0.71      5553\n",
      "\n",
      "Cross-Validation Scores: [0.68233387 0.66804755 0.68335735 0.67993516 0.67164986]\n",
      "Mean Cross-Validation Score: 0.6770647574253033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Get target names as a list of strings\n",
    "target_names = label_encoder.classes_.astype(str)  # Convert to strings if needed\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_combined, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aab3f6cf-7458-4e28-8dac-b90afabb49ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.72      0.72      1903\n",
      "     Neutral       0.64      0.62      0.63      1947\n",
      "    Positive       0.72      0.75      0.74      1703\n",
      "\n",
      "    accuracy                           0.70      5553\n",
      "   macro avg       0.70      0.70      0.70      5553\n",
      "weighted avg       0.69      0.70      0.69      5553\n",
      "\n",
      "Cross-Validation Scores: [0.67441023 0.64949568 0.67201009 0.6721902  0.66066282]\n",
      "Mean Cross-Validation Score: 0.665753803666113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Get target names as a list of strings\n",
    "target_names = label_encoder.classes_.astype(str)  # Convert to strings if needed\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a LogisticRegression model\n",
    "model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores1 = cross_val_score(model1, X_combined, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores1}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "85a6606a-a03b-4018-a526-65e0e36e0dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.63      0.62      1903\n",
      "     Neutral       0.54      0.53      0.54      1947\n",
      "    Positive       0.63      0.63      0.63      1703\n",
      "\n",
      "    accuracy                           0.60      5553\n",
      "   macro avg       0.60      0.60      0.60      5553\n",
      "weighted avg       0.60      0.60      0.60      5553\n",
      "\n",
      "Cross-Validation Scores: [0.56077796 0.55097262 0.56448127 0.56646254 0.52791787]\n",
      "Mean Cross-Validation Score: 0.5541224503617486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a DecisionTreeClassifier\n",
    "model2 = DecisionTreeClassifier(random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred2, target_names=label_encoder.classes_))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores2 = cross_val_score(model2, X_combined, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores2}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "043d4fa6-0028-4ae7-a7ae-897110c2a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.71      0.66      1903\n",
      "     Neutral       0.59      0.44      0.51      1947\n",
      "    Positive       0.65      0.73      0.68      1703\n",
      "\n",
      "    accuracy                           0.62      5553\n",
      "   macro avg       0.62      0.63      0.62      5553\n",
      "weighted avg       0.62      0.62      0.61      5553\n",
      "\n",
      "Cross-Validation Scores: [0.58688997 0.58123199 0.59528098 0.60122478 0.66462536]\n",
      "Mean Cross-Validation Score: 0.6058506163555697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Gaussian Naive Bayes\n",
    "model3 = GaussianNB()\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Gaussian Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, y_pred3, target_names=label_encoder.classes_))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores3 = cross_val_score(model3, X_combined, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores3}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "deff9377-17f6-4ebd-ac56-820658895cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.75      0.74      1903\n",
      "     Neutral       0.70      0.62      0.66      1947\n",
      "    Positive       0.72      0.80      0.76      1703\n",
      "\n",
      "    accuracy                           0.72      5553\n",
      "   macro avg       0.72      0.72      0.72      5553\n",
      "weighted avg       0.72      0.72      0.72      5553\n",
      "\n",
      "Cross-Validation Scores: [0.6904376  0.67489193 0.69650576 0.68587896 0.67182997]\n",
      "Mean Cross-Validation Score: 0.6839088459077344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM model\n",
    "model4 = SVC(random_state=42)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred4 = model4.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Support Vector Machine (SVM) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred4, target_names=label_encoder.classes_))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores4 = cross_val_score(model4, X_combined, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores4}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3819516-7915-49ff-b7fe-5b65ae14bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  39.6s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  39.5s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  39.8s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  37.0s\n",
      "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  37.6s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 3.5min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 3.4min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 3.2min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.3min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.3min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.3min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.2min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.3min\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  35.2s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  35.8s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  37.2s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  37.5s\n",
      "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=  35.6s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  44.8s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  44.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  44.3s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  45.3s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  45.5s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  36.8s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  37.2s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  37.1s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  37.5s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  37.5s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=  35.5s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=  37.7s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=  37.6s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=  36.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=  36.7s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  35.9s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  35.6s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  35.7s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  36.5s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=  35.7s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  52.5s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  53.5s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  53.5s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  52.4s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  53.5s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 1.1min\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=  53.3s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=  53.3s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=  54.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=  54.2s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=  54.2s\n",
      "[CV] END ..................C=0.1, gamma=0.001, kernel=linear; total time=  35.8s\n",
      "[CV] END ..................C=0.1, gamma=0.001, kernel=linear; total time=  35.3s\n",
      "[CV] END ..................C=0.1, gamma=0.001, kernel=linear; total time=  36.0s\n",
      "[CV] END ..................C=0.1, gamma=0.001, kernel=linear; total time=  36.3s\n",
      "[CV] END ..................C=0.1, gamma=0.001, kernel=linear; total time=  35.9s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time= 1.3min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time= 1.2min\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time= 1.2min\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time= 1.1min\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time= 1.1min\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time= 1.1min\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  41.5s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  40.9s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  41.3s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  42.2s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=  43.4s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 2.4min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 2.4min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 2.4min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 2.5min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 2.4min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.6min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.0min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.5min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.3min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.0min\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=  41.8s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=  43.5s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=  42.4s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=  44.8s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=  43.6s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  41.0s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  41.5s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  41.9s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  42.8s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=  44.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  40.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  40.5s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  42.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  41.4s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  40.8s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  37.9s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  38.8s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  39.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  40.1s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  42.9s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=  33.1s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=  32.5s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=  30.5s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=  33.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=  33.2s\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  41.1s\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  41.2s\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  40.8s\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  41.9s\n",
      "[CV] END .....................C=1, gamma=0.01, kernel=linear; total time=  41.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  41.8s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  41.9s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  42.7s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  42.6s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  42.9s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  58.2s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  57.2s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  58.2s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  58.3s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  58.2s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=  42.1s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=  42.1s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=  42.5s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=  42.7s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=  45.6s\n",
      "[CV] END ....................C=1, gamma=0.001, kernel=linear; total time=  41.6s\n",
      "[CV] END ....................C=1, gamma=0.001, kernel=linear; total time=  41.8s\n",
      "[CV] END ....................C=1, gamma=0.001, kernel=linear; total time=  41.0s\n",
      "[CV] END ....................C=1, gamma=0.001, kernel=linear; total time=  41.4s\n",
      "[CV] END ....................C=1, gamma=0.001, kernel=linear; total time=  41.3s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=  52.8s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=  52.9s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=  55.3s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=  54.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=  55.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=  53.5s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=  53.9s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=  54.4s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=  54.5s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=  53.9s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.2min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.2min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.3min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.2min\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time= 1.2min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 5.0min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 4.8min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 4.9min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 5.1min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 4.9min\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=  39.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=  39.3s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=  38.8s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=  40.6s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=  40.5s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.3min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time= 1.2min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  49.4s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  49.4s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  49.9s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  50.6s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  49.8s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 1.2min\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=  33.2s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=  32.8s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=  32.7s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=  32.9s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=  33.6s\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.2min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.2min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.3min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.2min\n",
      "[CV] END ....................C=10, gamma=0.01, kernel=linear; total time= 1.2min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  39.9s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  40.5s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  40.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  40.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  40.4s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  43.6s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  43.7s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  44.5s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  44.5s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  45.8s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=  42.1s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=  41.3s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=  41.2s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=  41.6s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=  40.7s\n",
      "[CV] END ...................C=10, gamma=0.001, kernel=linear; total time= 1.3min\n",
      "[CV] END ...................C=10, gamma=0.001, kernel=linear; total time= 1.4min\n",
      "[CV] END ...................C=10, gamma=0.001, kernel=linear; total time= 1.5min\n",
      "[CV] END ...................C=10, gamma=0.001, kernel=linear; total time= 1.4min\n",
      "[CV] END ...................C=10, gamma=0.001, kernel=linear; total time= 1.4min\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=  48.6s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=  47.4s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=  48.1s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=  47.4s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=  48.3s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time= 1.2min\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time= 1.2min\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=  45.2s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=  45.7s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=  45.5s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=  45.7s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=  46.1s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time= 5.6min\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time= 5.8min\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time= 5.9min\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time= 5.8min\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time= 5.8min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 3.9min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 3.8min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 4.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 3.9min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 3.8min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 5.5min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 5.4min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 5.5min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 5.6min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 5.4min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=  41.3s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=  41.2s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=  40.8s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=  42.8s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=  42.6s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time= 5.6min\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time= 5.6min\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time= 5.7min\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time= 5.6min\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time= 5.9min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.1min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.5min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 3.2min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 3.2min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 3.5min\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time= 6.2min\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time= 6.4min\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time= 6.7min\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time= 4.9min\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time= 3.1min\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=  33.7s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=  33.6s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=  34.5s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=  33.3s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=  33.7s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time= 4.9min\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time= 4.9min\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time= 5.0min\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time= 4.9min\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time= 4.9min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=  52.2s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=  52.4s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=  52.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=  53.3s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=  52.4s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=  37.8s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=  42.3s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=  51.6s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=  41.8s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=  39.8s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=  24.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=  23.7s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=  23.7s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=  24.0s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=  26.6s\n",
      "[CV] END ..................C=100, gamma=0.001, kernel=linear; total time= 5.1min\n",
      "[CV] END ..................C=100, gamma=0.001, kernel=linear; total time= 5.1min\n",
      "[CV] END ..................C=100, gamma=0.001, kernel=linear; total time= 5.2min\n",
      "[CV] END ..................C=100, gamma=0.001, kernel=linear; total time= 5.0min\n",
      "[CV] END ..................C=100, gamma=0.001, kernel=linear; total time= 5.0min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=  43.7s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=  42.1s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=  42.5s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=  42.8s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=  42.6s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time= 1.1min\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  40.9s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  45.5s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  41.7s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  39.5s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  41.3s\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Classification Report for the Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.75      0.75      1971\n",
      "     Neutral       0.68      0.63      0.66      1901\n",
      "    Positive       0.72      0.77      0.75      1680\n",
      "\n",
      "    accuracy                           0.72      5552\n",
      "   macro avg       0.71      0.72      0.72      5552\n",
      "weighted avg       0.71      0.72      0.71      5552\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred_best, target_names\u001b[38;5;241m=\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Perform cross-validation on the best model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m cv_scores_best \u001b[38;5;241m=\u001b[39m cross_val_score(best_model, X_combined, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-Validation Scores for the Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores_best\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cross-Validation Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(cv_scores_best)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],       # Regularization parameter\n",
    "    'gamma': [1, 0.1, 0.01, 0.001], # Kernel coefficient for 'rbf'\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'] # Kernel types\n",
    "}\n",
    "\n",
    "# Initialize the SVC model\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Classification Report for the Best Model:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=label_encoder.classes_))\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_scores_best = cross_val_score(best_model, X_combined, y, cv=5)\n",
    "\n",
    "print(f\"Cross-Validation Scores for the Best Model: {cv_scores_best}\")\n",
    "print(f\"Mean Cross-Validation Score: {np.mean(cv_scores_best)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70540fb-c5fe-4715-925e-15273afc092f",
   "metadata": {},
   "source": [
    "**SVM Worked best out of all, we could try/use Hugging Transformers and CNN,RNN but will increase the complexity of project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5329262-f011-4569-8510-42e91bea7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.75      0.75      1971\n",
      "     Neutral       0.68      0.63      0.66      1901\n",
      "    Positive       0.72      0.77      0.75      1680\n",
      "\n",
      "    accuracy                           0.72      5552\n",
      "   macro avg       0.71      0.72      0.72      5552\n",
      "weighted avg       0.71      0.72      0.71      5552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM model using the best hyperparameters\n",
    "best_params = {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "model4 = SVC(C=best_params['C'], gamma=best_params['gamma'], kernel=best_params['kernel'], random_state=42)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred4 = model4.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Support Vector Machine (SVM) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred4, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ae7bb-4bf4-4267-bb95-4d58acee7dd9",
   "metadata": {},
   "source": [
    "# PREDICTION ON A SAMPLE SENTENCE:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc0c33ee-c695-4d27-9142-b71e69d1caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(new_sentence):\n",
    "    # Convert new sentence to GloVe embedding\n",
    "    glove_feature = sentence_to_embedding(new_sentence, glove_embeddings).reshape(1, -1)\n",
    "    \n",
    "    # Extract VADER sentiment features\n",
    "    vader_feature = np.array(vader_sentiment_features(new_sentence)).reshape(1, -1)\n",
    "    \n",
    "    # Combine GloVe and VADER features\n",
    "    combined_feature = np.hstack([glove_feature, vader_feature])\n",
    "    \n",
    "    # Predict sentiment\n",
    "    prediction = model4.predict(combined_feature)\n",
    "    predicted_label = label_encoder.inverse_transform(prediction)\n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example of prediction\n",
    "new_sentence = \"What an interesting way to post.I am impressed.\"\n",
    "print(f\"Predicted sentiment: {predict_sentiment(new_sentence)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5076a29-2f14-4553-affc-a946c304e086",
   "metadata": {},
   "source": [
    "# PREDICTION ON A SAMPLE NEPALI SENTENCE:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27741c4e-e453-4367-b7ef-eb3dcbe7a179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "def translate_nepali_to_english(nepali_text):\n",
    "    # Translate the Nepali text to English\n",
    "    translation = translator.translate(nepali_text, src='ne', dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def predict_sentiment(new_sentence):\n",
    "    # Step 1: Translate Nepali to English\n",
    "    translated_sentence = translate_nepali_to_english(new_sentence)\n",
    "    \n",
    "    # Step 2: Convert the translated sentence to GloVe embedding\n",
    "    glove_feature = sentence_to_embedding(translated_sentence, glove_embeddings).reshape(1, -1)\n",
    "    \n",
    "    # Step 3: Extract VADER sentiment features from the translated sentence\n",
    "    vader_feature = np.array(vader_sentiment_features(translated_sentence)).reshape(1, -1)\n",
    "    \n",
    "    # Step 4: Combine GloVe and VADER features\n",
    "    combined_feature = np.hstack([glove_feature, vader_feature])\n",
    "    \n",
    "    # Step 5: Predict sentiment\n",
    "    prediction = model4.predict(combined_feature)\n",
    "    predicted_label = label_encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example of prediction with Nepali text\n",
    "new_sentence = \"तपाईंको पोस्ट धेरै रोचक छ। म प्रभावित भएँ।\"\n",
    "print(f\"Predicted sentiment: {predict_sentiment(new_sentence)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a3066-c4a8-4861-9e0d-e5dce05bf564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
